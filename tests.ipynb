{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is described path of selecting and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import catboost\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import gensim.downloader\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load and prepare code (initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train_path = os.path.join(path, 'train')\n",
    "    test_path = os.path.join(path, 'test')\n",
    "\n",
    "    train_pos_path = os.path.join(train_path, 'pos')\n",
    "    train_neg_path = os.path.join(train_path, 'neg')\n",
    "\n",
    "    test_pos_path = os.path.join(test_path, 'pos')\n",
    "    test_neg_path = os.path.join(test_path, 'neg')\n",
    "\n",
    "    train_pos = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(train_pos_path, x)] for x in os.listdir(train_pos_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    train_pos[\"Full text\"] = train_pos.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    train_neg = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(train_neg_path, x)] for x in os.listdir(train_neg_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    train_neg[\"Full text\"] = train_neg.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    test_pos = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(test_pos_path, x)] for x in os.listdir(test_pos_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    test_pos[\"Full text\"] = test_pos.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    test_neg = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(test_neg_path, x)] for x in os.listdir(test_neg_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    test_neg[\"Full text\"] = test_neg.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    train = pd.concat([train_pos, train_neg])#.sample(frac=1)\n",
    "    test = pd.concat([test_pos, test_neg])#.sample(frac=1)\n",
    "\n",
    "    train = train.drop(columns=[\"Full path\"])\n",
    "\n",
    "    test = test.drop(columns=[\"Full path\"])\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data('aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tested model - BERT-base + CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ded/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model.cuda()\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['Full text'].reset_index(drop=True)\n",
    "y = train_data['Mark'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    wnl = nltk.WordNetLemmatizer()  \n",
    "    x = ' '.join([wnl.lemmatize(word) for word in word_tokenize(x.lower()) if (word not in stopwords_set) and (word not in string.punctuation)])\n",
    "    t = tokenizer(x, padding=True, truncation=True, return_tensors='pt')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = pd.concat([pd.DataFrame(X.apply(lambda x: tokenize(x))), y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full text</th>\n",
       "      <th>Mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Full text  Mark\n",
       "0      [input_ids, token_type_ids, attention_mask]     7\n",
       "1      [input_ids, token_type_ids, attention_mask]     7\n",
       "2      [input_ids, token_type_ids, attention_mask]     9\n",
       "3      [input_ids, token_type_ids, attention_mask]     7\n",
       "4      [input_ids, token_type_ids, attention_mask]     7\n",
       "...                                            ...   ...\n",
       "24995  [input_ids, token_type_ids, attention_mask]     1\n",
       "24996  [input_ids, token_type_ids, attention_mask]     3\n",
       "24997  [input_ids, token_type_ids, attention_mask]     3\n",
       "24998  [input_ids, token_type_ids, attention_mask]     2\n",
       "24999  [input_ids, token_type_ids, attention_mask]     3\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "labels = []\n",
    "for index, row in tokenized_train.iterrows():\n",
    "    row_data = row[\"Full text\"]\n",
    "\n",
    "    labels.append(row[\"Mark\"])\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in row_data.items()})\n",
    "        embedding = model_output.last_hidden_state[:, 0, :]\n",
    "        embedding = torch.nn.functional.normalize(embedding)\n",
    "    embeddings.append(embedding[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(embeddings)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023313</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>-0.003292</td>\n",
       "      <td>-0.015695</td>\n",
       "      <td>-0.015810</td>\n",
       "      <td>0.013644</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>-0.029376</td>\n",
       "      <td>-0.010400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002764</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.020660</td>\n",
       "      <td>0.025957</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>-0.019887</td>\n",
       "      <td>-0.019603</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>-0.006124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000374</td>\n",
       "      <td>0.008638</td>\n",
       "      <td>0.026132</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>-0.019049</td>\n",
       "      <td>-0.029332</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>-0.010181</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>-0.050926</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.015874</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>-0.017069</td>\n",
       "      <td>-0.030555</td>\n",
       "      <td>0.015804</td>\n",
       "      <td>0.011660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.014271</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.022101</td>\n",
       "      <td>-0.013482</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.037497</td>\n",
       "      <td>0.013218</td>\n",
       "      <td>0.010021</td>\n",
       "      <td>-0.043483</td>\n",
       "      <td>-0.011192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020231</td>\n",
       "      <td>-0.019172</td>\n",
       "      <td>-0.012584</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>-0.024842</td>\n",
       "      <td>-0.023261</td>\n",
       "      <td>-0.027866</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.024729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.019618</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>-0.008883</td>\n",
       "      <td>-0.017093</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>0.016084</td>\n",
       "      <td>-0.015722</td>\n",
       "      <td>-0.009096</td>\n",
       "      <td>-0.009373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>-0.042235</td>\n",
       "      <td>-0.016593</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.010242</td>\n",
       "      <td>-0.002852</td>\n",
       "      <td>-0.035666</td>\n",
       "      <td>-0.010547</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.013186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.014457</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.022858</td>\n",
       "      <td>-0.018852</td>\n",
       "      <td>-0.013125</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>-0.022748</td>\n",
       "      <td>-0.011535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.057107</td>\n",
       "      <td>-0.011843</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>-0.023100</td>\n",
       "      <td>-0.031015</td>\n",
       "      <td>0.031687</td>\n",
       "      <td>-0.012271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>-0.015981</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>-0.024360</td>\n",
       "      <td>-0.002777</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>-0.010962</td>\n",
       "      <td>-0.006387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.047536</td>\n",
       "      <td>-0.010368</td>\n",
       "      <td>0.015290</td>\n",
       "      <td>0.023339</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>-0.027676</td>\n",
       "      <td>-0.013931</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>-0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>-0.002732</td>\n",
       "      <td>-0.002021</td>\n",
       "      <td>0.024036</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>-0.026290</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>-0.024859</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002507</td>\n",
       "      <td>-0.045069</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.023227</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>-0.018414</td>\n",
       "      <td>-0.018029</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.011211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.022319</td>\n",
       "      <td>0.020631</td>\n",
       "      <td>-0.012436</td>\n",
       "      <td>-0.026644</td>\n",
       "      <td>-0.023603</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>-0.001886</td>\n",
       "      <td>-0.005810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>-0.033208</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.014477</td>\n",
       "      <td>-0.021732</td>\n",
       "      <td>-0.038327</td>\n",
       "      <td>-0.007977</td>\n",
       "      <td>0.004688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>-0.023091</td>\n",
       "      <td>-0.027680</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>-0.037787</td>\n",
       "      <td>0.009192</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>-0.004543</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>-0.013433</td>\n",
       "      <td>-0.029221</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.007664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>-0.006602</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>-0.001567</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>-0.020376</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>0.025730</td>\n",
       "      <td>-0.020245</td>\n",
       "      <td>-0.005844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017497</td>\n",
       "      <td>-0.027944</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.028591</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>-0.023213</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>-0.013082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.023313  0.005784  0.034300 -0.003292 -0.015695 -0.015810  0.013644   \n",
       "1     -0.000374  0.008638  0.026132  0.002072 -0.019049 -0.029332  0.011774   \n",
       "2     -0.014271  0.008412  0.022101 -0.013482  0.000115 -0.037497  0.013218   \n",
       "3     -0.019618  0.000270  0.000530 -0.008883 -0.017093 -0.014391  0.016084   \n",
       "4     -0.014457  0.004122  0.022858 -0.018852 -0.013125  0.001092  0.010123   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24995 -0.015981  0.006766  0.012822  0.003048 -0.024360 -0.002777  0.007232   \n",
       "24996 -0.002732 -0.002021  0.024036  0.002596 -0.026290 -0.028555  0.009510   \n",
       "24997  0.002663 -0.022319  0.020631 -0.012436 -0.026644 -0.023603  0.027566   \n",
       "24998  0.014351  0.010978  0.012884 -0.006181 -0.023091 -0.027680  0.008688   \n",
       "24999 -0.006602  0.017689  0.007408 -0.001567 -0.022781 -0.020376  0.025251   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "0     -0.008273 -0.029376 -0.010400  ... -0.002764 -0.049534 -0.020660   \n",
       "1      0.004022 -0.010181 -0.000215  ...  0.003852 -0.050926  0.002099   \n",
       "2      0.010021 -0.043483 -0.011192  ... -0.020231 -0.019172 -0.012584   \n",
       "3     -0.015722 -0.009096 -0.009373  ... -0.005951 -0.042235 -0.016593   \n",
       "4      0.012368 -0.022748 -0.011535  ...  0.012695 -0.057107 -0.011843   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24995  0.002981 -0.010962 -0.006387  ... -0.004255 -0.047536 -0.010368   \n",
       "24996  0.010641 -0.024859  0.002800  ... -0.002507 -0.045069 -0.007849   \n",
       "24997  0.011035 -0.001886 -0.005810  ...  0.002567 -0.033208  0.005981   \n",
       "24998  0.027690  0.004729  0.008481  ...  0.006642 -0.037787  0.009192   \n",
       "24999  0.025730 -0.020245 -0.005844  ... -0.017497 -0.027944 -0.000810   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0      0.025957  0.012989  0.017903 -0.019887 -0.019603  0.017014 -0.006124  \n",
       "1      0.015874 -0.007511  0.028708 -0.017069 -0.030555  0.015804  0.011660  \n",
       "2      0.023629  0.025449 -0.024842 -0.023261 -0.027866  0.004701  0.024729  \n",
       "3      0.003059  0.010242 -0.002852 -0.035666 -0.010547  0.017022  0.013186  \n",
       "4      0.000589 -0.014636  0.002652 -0.023100 -0.031015  0.031687 -0.012271  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24995  0.015290  0.023339  0.020859 -0.027676 -0.013931  0.018225 -0.001410  \n",
       "24996  0.023227  0.006252  0.019126 -0.018414 -0.018029  0.020640  0.011211  \n",
       "24997  0.023733  0.002665  0.014477 -0.021732 -0.038327 -0.007977  0.004688  \n",
       "24998  0.016764 -0.004543  0.030645 -0.013433 -0.029221  0.022331  0.007664  \n",
       "24999  0.003169  0.028591  0.032073 -0.005139 -0.023213  0.010624 -0.013082  \n",
       "\n",
       "[25000 rows x 768 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_eval, y_test, y_eval = train_test_split(X_test, y_test, test_size=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train = catboost.Pool(data=X_train, label=y_train)\n",
    "pool_eval = catboost.Pool(data=X_eval, label=y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb = catboost.CatBoostRegressor(verbose=100, task_type=\"GPU\", devices=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04758bee23714ef69d53d48692bca96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.085827\n",
      "0:\tlearn: 3.4024723\ttest: 3.4387170\tbest: 3.4387170 (0)\ttotal: 183ms\tremaining: 3m 3s\n",
      "100:\tlearn: 2.5369648\ttest: 2.7163700\tbest: 2.7163700 (100)\ttotal: 9.35s\tremaining: 1m 23s\n",
      "200:\tlearn: 2.4014354\ttest: 2.6825165\tbest: 2.6825165 (200)\ttotal: 17.3s\tremaining: 1m 8s\n",
      "300:\tlearn: 2.2987353\ttest: 2.6643957\tbest: 2.6643957 (300)\ttotal: 25.1s\tremaining: 58.2s\n",
      "400:\tlearn: 2.2113170\ttest: 2.6536597\tbest: 2.6535752 (399)\ttotal: 32.9s\tremaining: 49.1s\n",
      "500:\tlearn: 2.1345129\ttest: 2.6473017\tbest: 2.6470715 (497)\ttotal: 40.7s\tremaining: 40.5s\n",
      "600:\tlearn: 2.0649850\ttest: 2.6464838\tbest: 2.6461021 (597)\ttotal: 49.4s\tremaining: 32.8s\n",
      "700:\tlearn: 2.0047091\ttest: 2.6393449\tbest: 2.6391107 (699)\ttotal: 58s\tremaining: 24.7s\n",
      "800:\tlearn: 1.9474480\ttest: 2.6359915\tbest: 2.6356588 (793)\ttotal: 1m 6s\tremaining: 16.6s\n",
      "900:\tlearn: 1.8964684\ttest: 2.6329381\tbest: 2.6329381 (900)\ttotal: 1m 14s\tremaining: 8.22s\n",
      "999:\tlearn: 1.8466990\ttest: 2.6295149\tbest: 2.6287587 (996)\ttotal: 1m 22s\tremaining: 0us\n",
      "bestTest = 2.628758749\n",
      "bestIteration = 996\n",
      "Shrink model to first 997 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7ff0f863e6f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb.fit(pool_train, eval_set=pool_eval, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45280708695702954"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model did not show good results. Next step - try GridCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data('aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['Full text'].reset_index(drop=True)\n",
    "y = train_data['Mark'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = pd.concat([pd.DataFrame(X.apply(lambda x: tokenize(x))), y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full text</th>\n",
       "      <th>Mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Full text  Mark\n",
       "0      [input_ids, token_type_ids, attention_mask]     7\n",
       "1      [input_ids, token_type_ids, attention_mask]     7\n",
       "2      [input_ids, token_type_ids, attention_mask]     9\n",
       "3      [input_ids, token_type_ids, attention_mask]     7\n",
       "4      [input_ids, token_type_ids, attention_mask]     7\n",
       "...                                            ...   ...\n",
       "24995  [input_ids, token_type_ids, attention_mask]     1\n",
       "24996  [input_ids, token_type_ids, attention_mask]     3\n",
       "24997  [input_ids, token_type_ids, attention_mask]     3\n",
       "24998  [input_ids, token_type_ids, attention_mask]     2\n",
       "24999  [input_ids, token_type_ids, attention_mask]     3\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "labels = []\n",
    "for index, row in tokenized_train.iterrows():\n",
    "    row_data = row[\"Full text\"]\n",
    "\n",
    "    labels.append(row[\"Mark\"])\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in row_data.items()})\n",
    "        embedding = model_output.last_hidden_state[:, 0, :]\n",
    "        embedding = torch.nn.functional.normalize(embedding)\n",
    "    embeddings.append(embedding[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(embeddings)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023313</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>-0.003292</td>\n",
       "      <td>-0.015695</td>\n",
       "      <td>-0.015810</td>\n",
       "      <td>0.013644</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>-0.029376</td>\n",
       "      <td>-0.010400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002764</td>\n",
       "      <td>-0.049534</td>\n",
       "      <td>-0.020660</td>\n",
       "      <td>0.025957</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>-0.019887</td>\n",
       "      <td>-0.019603</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>-0.006124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000374</td>\n",
       "      <td>0.008638</td>\n",
       "      <td>0.026132</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>-0.019049</td>\n",
       "      <td>-0.029332</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>-0.010181</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>-0.050926</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.015874</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>-0.017069</td>\n",
       "      <td>-0.030555</td>\n",
       "      <td>0.015804</td>\n",
       "      <td>0.011660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.014271</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.022101</td>\n",
       "      <td>-0.013482</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.037497</td>\n",
       "      <td>0.013218</td>\n",
       "      <td>0.010021</td>\n",
       "      <td>-0.043483</td>\n",
       "      <td>-0.011192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020231</td>\n",
       "      <td>-0.019172</td>\n",
       "      <td>-0.012584</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>-0.024842</td>\n",
       "      <td>-0.023261</td>\n",
       "      <td>-0.027866</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.024729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.019618</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>-0.008883</td>\n",
       "      <td>-0.017093</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>0.016084</td>\n",
       "      <td>-0.015722</td>\n",
       "      <td>-0.009096</td>\n",
       "      <td>-0.009373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>-0.042235</td>\n",
       "      <td>-0.016593</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.010242</td>\n",
       "      <td>-0.002852</td>\n",
       "      <td>-0.035666</td>\n",
       "      <td>-0.010547</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.013186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.014457</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.022858</td>\n",
       "      <td>-0.018852</td>\n",
       "      <td>-0.013125</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>-0.022748</td>\n",
       "      <td>-0.011535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.057107</td>\n",
       "      <td>-0.011843</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>-0.023100</td>\n",
       "      <td>-0.031015</td>\n",
       "      <td>0.031687</td>\n",
       "      <td>-0.012271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>-0.015981</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>-0.024360</td>\n",
       "      <td>-0.002777</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>-0.010962</td>\n",
       "      <td>-0.006387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.047536</td>\n",
       "      <td>-0.010368</td>\n",
       "      <td>0.015290</td>\n",
       "      <td>0.023339</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>-0.027676</td>\n",
       "      <td>-0.013931</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>-0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>-0.002732</td>\n",
       "      <td>-0.002021</td>\n",
       "      <td>0.024036</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>-0.026290</td>\n",
       "      <td>-0.028555</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>-0.024859</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002507</td>\n",
       "      <td>-0.045069</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.023227</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>-0.018414</td>\n",
       "      <td>-0.018029</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.011211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.022319</td>\n",
       "      <td>0.020631</td>\n",
       "      <td>-0.012436</td>\n",
       "      <td>-0.026644</td>\n",
       "      <td>-0.023603</td>\n",
       "      <td>0.027566</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>-0.001886</td>\n",
       "      <td>-0.005810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>-0.033208</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.014477</td>\n",
       "      <td>-0.021732</td>\n",
       "      <td>-0.038327</td>\n",
       "      <td>-0.007977</td>\n",
       "      <td>0.004688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.014351</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>-0.023091</td>\n",
       "      <td>-0.027680</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>-0.037787</td>\n",
       "      <td>0.009192</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>-0.004543</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>-0.013433</td>\n",
       "      <td>-0.029221</td>\n",
       "      <td>0.022331</td>\n",
       "      <td>0.007664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>-0.006602</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>-0.001567</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>-0.020376</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>0.025730</td>\n",
       "      <td>-0.020245</td>\n",
       "      <td>-0.005844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017497</td>\n",
       "      <td>-0.027944</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>0.028591</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>-0.023213</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>-0.013082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.023313  0.005784  0.034300 -0.003292 -0.015695 -0.015810  0.013644   \n",
       "1     -0.000374  0.008638  0.026132  0.002072 -0.019049 -0.029332  0.011774   \n",
       "2     -0.014271  0.008412  0.022101 -0.013482  0.000115 -0.037497  0.013218   \n",
       "3     -0.019618  0.000270  0.000530 -0.008883 -0.017093 -0.014391  0.016084   \n",
       "4     -0.014457  0.004122  0.022858 -0.018852 -0.013125  0.001092  0.010123   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24995 -0.015981  0.006766  0.012822  0.003048 -0.024360 -0.002777  0.007232   \n",
       "24996 -0.002732 -0.002021  0.024036  0.002596 -0.026290 -0.028555  0.009510   \n",
       "24997  0.002663 -0.022319  0.020631 -0.012436 -0.026644 -0.023603  0.027566   \n",
       "24998  0.014351  0.010978  0.012884 -0.006181 -0.023091 -0.027680  0.008688   \n",
       "24999 -0.006602  0.017689  0.007408 -0.001567 -0.022781 -0.020376  0.025251   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "0     -0.008273 -0.029376 -0.010400  ... -0.002764 -0.049534 -0.020660   \n",
       "1      0.004022 -0.010181 -0.000215  ...  0.003852 -0.050926  0.002099   \n",
       "2      0.010021 -0.043483 -0.011192  ... -0.020231 -0.019172 -0.012584   \n",
       "3     -0.015722 -0.009096 -0.009373  ... -0.005951 -0.042235 -0.016593   \n",
       "4      0.012368 -0.022748 -0.011535  ...  0.012695 -0.057107 -0.011843   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24995  0.002981 -0.010962 -0.006387  ... -0.004255 -0.047536 -0.010368   \n",
       "24996  0.010641 -0.024859  0.002800  ... -0.002507 -0.045069 -0.007849   \n",
       "24997  0.011035 -0.001886 -0.005810  ...  0.002567 -0.033208  0.005981   \n",
       "24998  0.027690  0.004729  0.008481  ...  0.006642 -0.037787  0.009192   \n",
       "24999  0.025730 -0.020245 -0.005844  ... -0.017497 -0.027944 -0.000810   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0      0.025957  0.012989  0.017903 -0.019887 -0.019603  0.017014 -0.006124  \n",
       "1      0.015874 -0.007511  0.028708 -0.017069 -0.030555  0.015804  0.011660  \n",
       "2      0.023629  0.025449 -0.024842 -0.023261 -0.027866  0.004701  0.024729  \n",
       "3      0.003059  0.010242 -0.002852 -0.035666 -0.010547  0.017022  0.013186  \n",
       "4      0.000589 -0.014636  0.002652 -0.023100 -0.031015  0.031687 -0.012271  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24995  0.015290  0.023339  0.020859 -0.027676 -0.013931  0.018225 -0.001410  \n",
       "24996  0.023227  0.006252  0.019126 -0.018414 -0.018029  0.020640  0.011211  \n",
       "24997  0.023733  0.002665  0.014477 -0.021732 -0.038327 -0.007977  0.004688  \n",
       "24998  0.016764 -0.004543  0.030645 -0.013433 -0.029221  0.022331  0.007664  \n",
       "24999  0.003169  0.028591  0.032073 -0.005139 -0.023213  0.010624 -0.013082  \n",
       "\n",
       "[25000 rows x 768 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train = catboost.Pool(data=X_train, label=y_train)\n",
    "pool_test = catboost.Pool(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5fa7f5195243558c4ae2353ddbde2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 6.4303835\ttest: 6.4378137\tbest: 6.4378137 (0)\ttotal: 35.2ms\tremaining: 35.2s\n",
      "100:\tlearn: 3.6992963\ttest: 3.7127893\tbest: 3.7127893 (100)\ttotal: 3.48s\tremaining: 31s\n",
      "200:\tlearn: 3.0296742\ttest: 3.0502958\tbest: 3.0502958 (200)\ttotal: 6.87s\tremaining: 27.3s\n",
      "300:\tlearn: 2.8471839\ttest: 2.8752935\tbest: 2.8752935 (300)\ttotal: 10.3s\tremaining: 23.9s\n",
      "400:\tlearn: 2.7697081\ttest: 2.8048348\tbest: 2.8048348 (400)\ttotal: 13.6s\tremaining: 20.2s\n",
      "500:\tlearn: 2.7219422\ttest: 2.7627307\tbest: 2.7627307 (500)\ttotal: 16.9s\tremaining: 16.8s\n",
      "600:\tlearn: 2.6872995\ttest: 2.7345162\tbest: 2.7345162 (600)\ttotal: 20.4s\tremaining: 13.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m grid_model \u001b[38;5;241m=\u001b[39m catboost\u001b[38;5;241m.\u001b[39mCatBoostRegressor(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m7\u001b[39m],\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m \u001b[43mgrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/catboost/core.py:4211\u001b[0m, in \u001b[0;36mCatBoost.grid_search\u001b[0;34m(self, param_grid, X, y, cv, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   4208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grid[key], Iterable):\n\u001b[1;32m   4209\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter grid value is not iterable (key=\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m, value=\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key, grid[key]))\n\u001b[0;32m-> 4211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tune_hyperparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_random_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_random_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalc_cv_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalc_cv_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4214\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_by_train_test_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_by_train_test_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_cerr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/catboost/core.py:4100\u001b[0m, in \u001b[0;36mCatBoost._tune_hyperparams\u001b[0;34m(self, param_grid, X, y, cv, n_iter, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   4097\u001b[0m     stratified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(loss_function, STRING_TYPES) \u001b[38;5;129;01mand\u001b[39;00m is_cv_stratified_objective(loss_function)\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHyperparameters search plot\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(params)]):\n\u001b[0;32m-> 4100\u001b[0m     cv_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tune_hyperparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_pool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition_random_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_by_train_test_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalc_cv_statistics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m   4104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refit:\n\u001b[1;32m   4107\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fitted()\n",
      "File \u001b[0;32m_catboost.pyx:5524\u001b[0m, in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:5562\u001b[0m, in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_model = catboost.CatBoostRegressor(verbose=100, task_type=\"GPU\", devices=\"0\")\n",
    "\n",
    "param_dist = {\n",
    "    'iterations': [10**i for i in range(3, 4)],\n",
    "    'learning_rate': np.linspace(0.01, 0.2, 5),\n",
    "    'depth': [4, 7],\n",
    "}\n",
    "\n",
    "grid_model.grid_search(param_grid=param_dist, X=pool_train, refit=True, verbose=100, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridCV give no results, use default model params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried some other tokenizers, they are about the same in quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_word2vec(text):\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    wnl = nltk.WordNetLemmatizer()  \n",
    "    x = [wnl.lemmatize(word) for word in word_tokenize(text.lower()) if (word not in stopwords_set) and (word not in string.punctuation)]\n",
    "    embeddings =[]\n",
    "    try:\n",
    "        embeddings = [vectors[word] for word in x if word in vectors]\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "    \n",
    "    if embeddings:\n",
    "        # Усредняем эмбеддинги по каждой координате\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for fea in X:\n",
    "    features.append(tokenize_word2vec(fea))\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02600561,  0.0981196 , -0.06407259, ...,  0.12736613,\n",
       "        -0.00164985, -0.17884152],\n",
       "       [-0.01128365,  0.04778509, -0.00855022, ...,  0.15964073,\n",
       "        -0.07756658, -0.11727268],\n",
       "       [-0.09386162, -0.004718  , -0.13068983, ...,  0.07304465,\n",
       "         0.01082644, -0.05915766],\n",
       "       ...,\n",
       "       [ 0.03862862,  0.14663793,  0.04064982, ...,  0.15826869,\n",
       "        -0.0895642 , -0.15713716],\n",
       "       [ 0.08103628,  0.24486575, -0.07451502, ...,  0.193327  ,\n",
       "         0.01319468, -0.12568967],\n",
       "       [-0.00223744,  0.05071123, -0.05749378, ...,  0.19464792,\n",
       "        -0.04045384, -0.0445789 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_eval, y_test, y_eval = train_test_split(X_test, y_test, test_size=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train = catboost.Pool(data=X_train, label=y_train)\n",
    "pool_eval = catboost.Pool(data=X_eval, label=y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_w2v_p = catboost.CatBoostRegressor(verbose=100, task_type=\"GPU\", devices=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7952f2be9c645398eb6a65a7a623728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.085827\n",
      "0:\tlearn: 3.3996381\ttest: 3.4011909\tbest: 3.4011909 (0)\ttotal: 39.6ms\tremaining: 39.5s\n",
      "100:\tlearn: 2.4421111\ttest: 2.5322818\tbest: 2.5322818 (100)\ttotal: 5.06s\tremaining: 45s\n",
      "200:\tlearn: 2.3169151\ttest: 2.4741154\tbest: 2.4741154 (200)\ttotal: 10.3s\tremaining: 40.8s\n",
      "300:\tlearn: 2.2276512\ttest: 2.4568829\tbest: 2.4568829 (300)\ttotal: 14.9s\tremaining: 34.5s\n",
      "400:\tlearn: 2.1560159\ttest: 2.4445090\tbest: 2.4442639 (385)\ttotal: 19.3s\tremaining: 28.8s\n",
      "500:\tlearn: 2.0918555\ttest: 2.4334249\tbest: 2.4334249 (500)\ttotal: 23.9s\tremaining: 23.8s\n",
      "600:\tlearn: 2.0361725\ttest: 2.4259651\tbest: 2.4259651 (600)\ttotal: 28.4s\tremaining: 18.9s\n",
      "700:\tlearn: 1.9886918\ttest: 2.4197088\tbest: 2.4191582 (675)\ttotal: 33s\tremaining: 14.1s\n",
      "800:\tlearn: 1.9436407\ttest: 2.4172771\tbest: 2.4172771 (800)\ttotal: 37.5s\tremaining: 9.32s\n",
      "900:\tlearn: 1.9002121\ttest: 2.4130885\tbest: 2.4121632 (889)\ttotal: 42.2s\tremaining: 4.63s\n",
      "999:\tlearn: 1.8644514\ttest: 2.4126275\tbest: 2.4113878 (956)\ttotal: 46.6s\tremaining: 0us\n",
      "bestTest = 2.411387778\n",
      "bestIteration = 956\n",
      "Shrink model to first 957 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7ff031f787d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_w2v_p.fit(pool_train, eval_set=pool_eval, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5007320811453071"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_w2v_p.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_word2vec2train(text):\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    wnl = nltk.WordNetLemmatizer()  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    x = [wnl.lemmatize(word) for word in word_tokenize(text.lower()) if (word not in stopwords_set) and (word not in string.punctuation)]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.apply(lambda x: tokenize_word2vec2train(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=features, vector_size=200, window=5, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word2vec_self(text):\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    wnl = nltk.WordNetLemmatizer()  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    x = [wnl.lemmatize(word) for word in word_tokenize(text.lower()) if (word not in stopwords_set) and (word not in string.punctuation)]\n",
    "    embeddings =[]\n",
    "    try:\n",
    "        embeddings = [model.wv[word] for word in x if word in model.wv]\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "    \n",
    "    if embeddings:\n",
    "        # Усредняем эмбеддинги по каждой координате\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for fea in X:\n",
    "    features.append(tokenize_word2vec_self(fea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(features)\n",
    "labels = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_eval, y_test, y_eval = train_test_split(X_test, y_test, test_size=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train = catboost.Pool(data=X_train, label=y_train)\n",
    "pool_eval = catboost.Pool(data=X_eval, label=y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_w2v_s = catboost.CatBoostRegressor(verbose=100, task_type=\"GPU\", devices=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625b49965a364ebea27e4acbf7365360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.085827\n",
      "0:\tlearn: 3.3905256\ttest: 3.3975001\tbest: 3.3975001 (0)\ttotal: 40.4ms\tremaining: 40.3s\n",
      "100:\tlearn: 2.5049517\ttest: 2.6498708\tbest: 2.6496348 (99)\ttotal: 4.71s\tremaining: 42s\n",
      "200:\tlearn: 2.3940438\ttest: 2.6116332\tbest: 2.6116332 (200)\ttotal: 9.01s\tremaining: 35.8s\n",
      "300:\tlearn: 2.3143641\ttest: 2.5935216\tbest: 2.5932178 (297)\ttotal: 13.4s\tremaining: 31s\n",
      "400:\tlearn: 2.2490555\ttest: 2.5803553\tbest: 2.5802408 (399)\ttotal: 17.3s\tremaining: 25.9s\n",
      "500:\tlearn: 2.1895755\ttest: 2.5712406\tbest: 2.5711503 (499)\ttotal: 21.3s\tremaining: 21.3s\n",
      "600:\tlearn: 2.1389664\ttest: 2.5689366\tbest: 2.5673231 (578)\ttotal: 25.3s\tremaining: 16.8s\n",
      "700:\tlearn: 2.0978136\ttest: 2.5658651\tbest: 2.5656923 (699)\ttotal: 29.2s\tremaining: 12.5s\n",
      "800:\tlearn: 2.0617054\ttest: 2.5626614\tbest: 2.5626239 (799)\ttotal: 33.2s\tremaining: 8.24s\n",
      "900:\tlearn: 2.0279008\ttest: 2.5599774\tbest: 2.5597374 (885)\ttotal: 37.1s\tremaining: 4.08s\n",
      "999:\tlearn: 1.9933167\ttest: 2.5574087\tbest: 2.5574087 (999)\ttotal: 41.1s\tremaining: 0us\n",
      "bestTest = 2.557408719\n",
      "bestIteration = 999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7ff0fa08e6f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_w2v_s.fit(pool_train, eval_set=pool_eval, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45533190914043264"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_w2v_s.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the quality, we will try an ensemble of models: two classifiers will complement the embedding vector with their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New data load func (Boolean features)\n",
    "def load_data(path):\n",
    "    train_path = os.path.join(path, 'train')\n",
    "    test_path = os.path.join(path, 'test')\n",
    "\n",
    "    train_pos_path = os.path.join(train_path, 'pos')\n",
    "    train_neg_path = os.path.join(train_path, 'neg')\n",
    "\n",
    "    test_pos_path = os.path.join(test_path, 'pos')\n",
    "    test_neg_path = os.path.join(test_path, 'neg')\n",
    "\n",
    "    train_pos = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(train_pos_path, x)] for x in os.listdir(train_pos_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    train_pos[\"Full text\"] = train_pos.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    train_neg = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(train_neg_path, x)] for x in os.listdir(train_neg_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    train_neg[\"Full text\"] = train_neg.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    test_pos = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(test_pos_path, x)] for x in os.listdir(test_pos_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    test_pos[\"Full text\"] = test_pos.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    test_neg = pd.DataFrame([[int(os.path.splitext(x)[0].split('_')[1]), os.path.join(test_neg_path, x)] for x in os.listdir(test_neg_path)], columns=[\"Mark\", \"Full path\"])\n",
    "    test_neg[\"Full text\"] = test_neg.apply(lambda x: load_text(x[\"Full path\"]), axis=1)\n",
    "\n",
    "    train = pd.concat([train_pos, train_neg])#.sample(frac=1)\n",
    "    test = pd.concat([test_pos, test_neg])#.sample(frac=1)\n",
    "\n",
    "    train = train.drop(columns=[\"Full path\"])\n",
    "    train['Positive'] = train['Mark'] >= 7\n",
    "    train['Negative'] = train[\"Mark\"] <= 4\n",
    "\n",
    "    test = test.drop(columns=[\"Full path\"])\n",
    "    test['Positive'] = test['Mark'] >= 7\n",
    "    test['Negative'] = test[\"Mark\"] <= 4\n",
    "\n",
    "\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data('aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['Full text'].reset_index(drop=True)\n",
    "y = train_data['Positive'].reset_index(drop=True)\n",
    "y_2 = train_data['Negative'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ded/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model.cuda()\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    wnl = nltk.WordNetLemmatizer()  \n",
    "    x = ' '.join([wnl.lemmatize(word) for word in word_tokenize(x.lower()) if (word not in stopwords_set) and (word not in string.punctuation)])\n",
    "    t = tokenizer(x, padding=True, truncation=True, return_tensors='pt')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_embeddings = pd.DataFrame(X.apply(lambda x: tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Full text\n",
       "0      [input_ids, token_type_ids, attention_mask]\n",
       "1      [input_ids, token_type_ids, attention_mask]\n",
       "2      [input_ids, token_type_ids, attention_mask]\n",
       "3      [input_ids, token_type_ids, attention_mask]\n",
       "4      [input_ids, token_type_ids, attention_mask]\n",
       "...                                            ...\n",
       "24995  [input_ids, token_type_ids, attention_mask]\n",
       "24996  [input_ids, token_type_ids, attention_mask]\n",
       "24997  [input_ids, token_type_ids, attention_mask]\n",
       "24998  [input_ids, token_type_ids, attention_mask]\n",
       "24999  [input_ids, token_type_ids, attention_mask]\n",
       "\n",
       "[25000 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for index, row in tokenized_embeddings.iterrows():\n",
    "    row_data = row[\"Full text\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in row_data.items()})\n",
    "        embedding = model_output.last_hidden_state[:, 0, :]\n",
    "        embedding = torch.nn.functional.normalize(embedding)\n",
    "    embeddings.append(embedding[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(embeddings)\n",
    "labels_pos = np.array(y)\n",
    "labels_neg = np.array(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_pos, y_test_pos = train_test_split(features, labels_pos, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_eval, y_test_pos, y_eval_pos = train_test_split(X_test, y_test_pos, test_size=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train_pos = catboost.Pool(data=X_train, label=y_train_pos)\n",
    "pool_eval_pos = catboost.Pool(data=X_test, label=y_test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_pos = catboost.CatBoostClassifier(verbose=100, task_type=\"GPU\", devices=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548fad1b710e427fba8894ae60e756ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0552\n",
      "0:\tlearn: 0.6807842\ttest: 0.6808469\tbest: 0.6808469 (0)\ttotal: 201ms\tremaining: 3m 20s\n",
      "100:\tlearn: 0.4528600\ttest: 0.4695452\tbest: 0.4695452 (100)\ttotal: 9.87s\tremaining: 1m 27s\n",
      "200:\tlearn: 0.4100656\ttest: 0.4495698\tbest: 0.4495698 (200)\ttotal: 18.7s\tremaining: 1m 14s\n",
      "300:\tlearn: 0.3803601\ttest: 0.4387194\tbest: 0.4386981 (298)\ttotal: 27.6s\tremaining: 1m 4s\n",
      "400:\tlearn: 0.3569461\ttest: 0.4331594\tbest: 0.4331594 (400)\ttotal: 36.3s\tremaining: 54.2s\n",
      "500:\tlearn: 0.3370018\ttest: 0.4303522\tbest: 0.4303522 (500)\ttotal: 45.4s\tremaining: 45.2s\n",
      "600:\tlearn: 0.3188604\ttest: 0.4272364\tbest: 0.4272024 (599)\ttotal: 54s\tremaining: 35.8s\n",
      "700:\tlearn: 0.3044974\ttest: 0.4254137\tbest: 0.4252311 (685)\ttotal: 1m 2s\tremaining: 26.7s\n",
      "800:\tlearn: 0.2910578\ttest: 0.4240233\tbest: 0.4240233 (800)\ttotal: 1m 12s\tremaining: 18s\n",
      "900:\tlearn: 0.2779351\ttest: 0.4223774\tbest: 0.4223774 (900)\ttotal: 1m 21s\tremaining: 8.95s\n",
      "999:\tlearn: 0.2664589\ttest: 0.4210852\tbest: 0.4210513 (998)\ttotal: 1m 30s\tremaining: 0us\n",
      "bestTest = 0.4210513102\n",
      "bestIteration = 998\n",
      "Shrink model to first 999 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f7d6d900920>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_pos.fit(pool_train_pos, eval_set=pool_eval_pos, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7985"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_pos.score(X_eval, y_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_pos.save_model(\"model_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_neg, y_test_neg = train_test_split(features, labels_neg, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_eval, y_test_neg, y_eval_neg = train_test_split(X_test, y_test_neg, test_size=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train_neg = catboost.Pool(data=X_train, label=y_train_neg)\n",
    "pool_eval_neg = catboost.Pool(data=X_test, label=y_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_neg = catboost.CatBoostClassifier(verbose=100, task_type=\"GPU\", devices=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d496da70b124475b69fc932d7dd9c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0552\n",
      "0:\tlearn: 0.6811795\ttest: 0.6817184\tbest: 0.6817184 (0)\ttotal: 108ms\tremaining: 1m 47s\n",
      "100:\tlearn: 0.4519076\ttest: 0.4823251\tbest: 0.4823251 (100)\ttotal: 9.62s\tremaining: 1m 25s\n",
      "200:\tlearn: 0.4087329\ttest: 0.4603979\tbest: 0.4603979 (200)\ttotal: 18.5s\tremaining: 1m 13s\n",
      "300:\tlearn: 0.3786442\ttest: 0.4501798\tbest: 0.4501798 (300)\ttotal: 26.9s\tremaining: 1m 2s\n",
      "400:\tlearn: 0.3546421\ttest: 0.4438354\tbest: 0.4438354 (400)\ttotal: 35.6s\tremaining: 53.2s\n",
      "500:\tlearn: 0.3343662\ttest: 0.4403437\tbest: 0.4403437 (500)\ttotal: 44s\tremaining: 43.8s\n",
      "600:\tlearn: 0.3168132\ttest: 0.4374014\tbest: 0.4373770 (598)\ttotal: 52.4s\tremaining: 34.8s\n",
      "700:\tlearn: 0.3019467\ttest: 0.4344915\tbest: 0.4344915 (700)\ttotal: 1m\tremaining: 25.9s\n",
      "800:\tlearn: 0.2870584\ttest: 0.4321711\tbest: 0.4321711 (800)\ttotal: 1m 9s\tremaining: 17.2s\n",
      "900:\tlearn: 0.2734155\ttest: 0.4301605\tbest: 0.4301494 (898)\ttotal: 1m 17s\tremaining: 8.53s\n",
      "999:\tlearn: 0.2613694\ttest: 0.4284285\tbest: 0.4284285 (999)\ttotal: 1m 26s\tremaining: 0us\n",
      "bestTest = 0.4284285075\n",
      "bestIteration = 999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f7d174ee030>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_neg.fit(pool_train_neg, eval_set=pool_eval_neg, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8105"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_neg.score(X_eval, y_eval_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_neg.save_model(\"model_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pos = ctb_pos.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_neg = ctb_neg.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_reg = pd.concat([features, pd.DataFrame(preds_pos.reshape(-1,1), columns=['Positive']), pd.DataFrame(preds_neg.reshape(-1,1), columns=['Negative'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_reg = train_data['Mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_to_reg, labels_to_reg, test_size=0.2, shuffle=True)\n",
    "X_test, X_eval, y_test, y_eval = train_test_split(X_test, y_test, test_size=0.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_train_reg = catboost.Pool(data=X_train, label=y_train)\n",
    "pool_eval_reg = catboost.Pool(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_reg = catboost.CatBoostRegressor(iterations=1000, depth=9, learning_rate=0.1, verbose=100, task_type=\"GPU\", devices=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d932d52fdcfd4c789f9ea2a68d18e0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.2515845\ttest: 3.2361623\tbest: 3.2361623 (0)\ttotal: 255ms\tremaining: 4m 14s\n",
      "100:\tlearn: 1.9118614\ttest: 2.0138049\tbest: 2.0138049 (100)\ttotal: 19.6s\tremaining: 2m 54s\n",
      "200:\tlearn: 1.7772236\ttest: 2.0118817\tbest: 2.0110549 (144)\ttotal: 41.4s\tremaining: 2m 44s\n",
      "300:\tlearn: 1.6646810\ttest: 2.0078367\tbest: 2.0069459 (283)\ttotal: 1m 3s\tremaining: 2m 28s\n",
      "400:\tlearn: 1.5638590\ttest: 2.0083558\tbest: 2.0067162 (371)\ttotal: 1m 27s\tremaining: 2m 10s\n",
      "500:\tlearn: 1.4832188\ttest: 2.0079974\tbest: 2.0067162 (371)\ttotal: 1m 50s\tremaining: 1m 50s\n",
      "600:\tlearn: 1.4052963\ttest: 2.0115264\tbest: 2.0067162 (371)\ttotal: 2m 13s\tremaining: 1m 28s\n",
      "700:\tlearn: 1.3394335\ttest: 2.0128295\tbest: 2.0067162 (371)\ttotal: 2m 37s\tremaining: 1m 7s\n",
      "800:\tlearn: 1.2757344\ttest: 2.0122037\tbest: 2.0067162 (371)\ttotal: 2m 58s\tremaining: 44.4s\n",
      "900:\tlearn: 1.2212923\ttest: 2.0140527\tbest: 2.0067162 (371)\ttotal: 3m 23s\tremaining: 22.4s\n",
      "999:\tlearn: 1.1720549\ttest: 2.0126745\tbest: 2.0067162 (371)\ttotal: 3m 47s\tremaining: 0us\n",
      "bestTest = 2.006716181\n",
      "bestIteration = 371\n",
      "Shrink model to first 372 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7f7d17d879e0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_reg.fit(pool_train_reg, eval_set=pool_eval_reg, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6556749285248333"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctb_reg.score(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of models gave an increase in quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
